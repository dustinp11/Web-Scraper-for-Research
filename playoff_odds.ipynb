{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5f2f82-93d8-4f27-9cce-f6e2e1541d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9708b681-ba27-4e46-a31c-46830484cb77",
   "metadata": {},
   "source": [
    "Note 1 : 2018 and after adds a column called \"Strength of Schedule\" which changes the index of \"Win division\", \"Clinch wildcard\", and \"make playoffs\". There is a need to load both datasets separately and then merge them. \n",
    "Note 2: 2022 and after has a new column \"clinch bye\". \n",
    "Note 3: Possible to speed this up by interacting with the calender menu and selecting a day. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d228ac-eab6-4805-9018-2633cdfdcc89",
   "metadata": {},
   "source": [
    "https://www.fangraphs.com/standings/playoff-odds/fg/wc?date=2014-03-31&dateDelta="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40642e4d-2e54-4822-b679-664db6e0276b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chrome_options \u001b[38;5;241m=\u001b[39m Options()\n\u001b[0;32m      2\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL:/chromedriver/chromedriver-win64/chromedriver.exe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Options' is not defined"
     ]
    }
   ],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "service = Service(\"L:/chromedriver/chromedriver-win64/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e4edb9-7555-4928-a4fb-ffa2c7247cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2173\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "start_date = datetime(2014, 3, 30)\n",
    "\n",
    "end_date = datetime.today()\n",
    "\n",
    "date_list = []\n",
    "\n",
    "\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_list.append(current_date.strftime('%Y-%m-%d'))\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(date_list.index(\"2020-03-11\"))\n",
    "url_wc = [\"https://www.fangraphs.com/standings/playoff-odds/fg/wc?date=\" + x + \"&dateDelta=\" for x in date_list]\n",
    "url_lg = [\"https://www.fangraphs.com/standings/playoff-odds/fg/lg?date=\" + x + \"&dateDelta=\" for x in date_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d15a156-fd48-483b-b35f-1c966c53dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the names of the teams + the headers for the csv\n",
    "mlb_teams = [\n",
    "    \"Diamondbacks\",\"Braves\", \"Orioles\", \"Red Sox\", \"White Sox\", \"Cubs\",\n",
    "    \"Reds\", \"Guardians\", \"Rockies\", \"Tigers\", \"Astros\", \"Royals\", \n",
    "    \"Angels\", \"Dodgers\", \"Marlins\", \"Brewers\", \"Twins\", \"Mets\", \n",
    "    \"Yankees\", \"Athletics\", \"Phillies\", \"Pirates\", \"Padres\", \"Giants\", \n",
    "    \"Mariners\", \"Cardinals\", \"Rays\", \"Rangers\", \"Blue Jays\", \"Nationals\"\n",
    "]\n",
    "\n",
    "headers = [\"year\", \"team\", \"game\", \"date\", \"win\", \"loss\", \"GB\", \"GBWC\",\n",
    "           \"win division\", \"clinch wildcard\", \"make playoffs\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67e9da5d-26b7-470b-b0f8-bca707cd5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(table, is_wildcard: bool) -> list: \n",
    "    \"\"\"Parses through the html given.\"\"\"\n",
    "    res = []\n",
    "    soup = BeautifulSoup(table, 'html.parser')\n",
    "    rows = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        info = row.find_all('td')\n",
    "        if is_wildcard:\n",
    "            name  = info[0].find('span', {'class': 'fullName'}).text.strip()\n",
    "            if \"+\" in info[4].text.strip():\n",
    "                res.append((name, \"up \" + info[4].text.strip()[1:]))\n",
    "            else:\n",
    "                res.append((name, info[4].text.strip()))\n",
    "        else:\n",
    "            team_info = [' ' for _ in range(11)]\n",
    "            for index, col in enumerate(info):\n",
    "                if index == 0: # name\n",
    "                    team_name = row.find('span', {'class': 'fullName'}).text.strip()\n",
    "                    team_info[1] = team_name\n",
    "                elif index == 1 or index == 2: # wins and losses \n",
    "                    team_info[index + 3] = col.text.strip()\n",
    "                elif index == 4: # gb\n",
    "                    team_info[6] = col.text.strip() \n",
    "                    \n",
    "                elif index == 8: # win division \n",
    "                    team_info[8] = col.text.strip()\n",
    "                    \n",
    "                elif index == 9: # clinch wildcard\n",
    "                    team_info[9] = col.text.strip()\n",
    "                    \n",
    "                elif index == 10: # make playoffs\n",
    "                    team_info[10] = col.text.strip()\n",
    "            res.append(team_info)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16468ec4-769b-443c-8f24-b16214c8bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_2018(table, is_wildcard: bool) -> list: \n",
    "    \"\"\"Parses through the html given.\"\"\"\n",
    "    res = []\n",
    "    soup = BeautifulSoup(table, 'html.parser')\n",
    "    rows = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        info = row.find_all('td')\n",
    "        if is_wildcard:\n",
    "            name  = info[0].find('span', {'class': 'fullName'}).text.strip()\n",
    "            if \"+\" in info[4].text.strip():\n",
    "                res.append((name, \"up \" + info[4].text.strip()[1:]))\n",
    "            else:\n",
    "                res.append((name, info[4].text.strip()))\n",
    "        else:\n",
    "            team_info = [' ' for _ in range(11)]\n",
    "            for index, col in enumerate(info):\n",
    "                if index == 0: # name\n",
    "                    team_name = row.find('span', {'class': 'fullName'}).text.strip()\n",
    "                    team_info[1] = team_name\n",
    "                elif index == 1 or index == 2: # wins and losses \n",
    "                    team_info[index + 3] = col.text.strip()\n",
    "                elif index == 4: # gb\n",
    "                    team_info[6] = col.text.strip() \n",
    "                    \n",
    "                elif index == 9: # win division \n",
    "                    team_info[8] = col.text.strip()\n",
    "                    \n",
    "                elif index == 10: # clinch wildcard\n",
    "                    team_info[9] = col.text.strip()\n",
    "                    \n",
    "                elif index == 11: # make playoffs\n",
    "                    team_info[10] = col.text.strip()\n",
    "            res.append(team_info)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337edd4f-b6bc-4c8b-96fe-68ac04c3ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html_2022(table, is_wildcard: bool) -> list: \n",
    "    \"\"\"Parses through the html given.\"\"\"\n",
    "    res = []\n",
    "    soup = BeautifulSoup(table, 'html.parser')\n",
    "    rows = soup.find('tbody').find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        info = row.find_all('td')\n",
    "        if is_wildcard:\n",
    "            name  = info[0].find('span', {'class': 'fullName'}).text.strip()\n",
    "            if \"+\" in info[4].text.strip():\n",
    "                res.append((name, \"up \" + info[4].text.strip()[1:]))\n",
    "            else:\n",
    "                res.append((name, info[4].text.strip()))\n",
    "        else:\n",
    "            team_info = [' ' for _ in range(11)]\n",
    "            for index, col in enumerate(info):\n",
    "                if index == 0: # name\n",
    "                    team_name = row.find('span', {'class': 'fullName'}).text.strip()\n",
    "                    team_info[1] = team_name\n",
    "                elif index == 1 or index == 2: # wins and losses \n",
    "                    team_info[index + 3] = col.text.strip()\n",
    "                elif index == 4: # gb\n",
    "                    team_info[6] = col.text.strip() \n",
    "                    \n",
    "                elif index == 9: # win division \n",
    "                    team_info[8] = col.text.strip()\n",
    "                    \n",
    "                elif index == 11: # clinch wildcard\n",
    "                    team_info[9] = col.text.strip()\n",
    "                    \n",
    "                elif index == 12: # make playoffs\n",
    "                    team_info[10] = col.text.strip()\n",
    "            res.append(team_info)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47368ac0-017d-422f-8f3d-26ac708c65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(lg, wc, date):\n",
    "    \"\"\"Combines the data gathered from wildcard and league.\"\"\"\n",
    "    \n",
    "    for team in lg:\n",
    "        games = int(lg[team][4]) + int(lg[team][5]) + 1\n",
    "        lg[team][0] = date[:4]\n",
    "        lg[team][2] = games\n",
    "        lg[team][3] = date\n",
    "        \n",
    "        if team in wc:\n",
    "            lg[team][7] = wc[team][1]\n",
    "    return lg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96cbbc0-44b2-4b0d-84dc-fbd82277005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(driver, is_wildcard: bool, year) -> dict:\n",
    "    \"\"\"Loads the page given url.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(driver, 8).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'playoff-odds-table')))\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        table_html = soup.find_all('table', 'playoff-odds-table')\n",
    "        AL_table_html = str(table_html[1])\n",
    "        NL_table_html = str(table_html[3])\n",
    "\n",
    "        if year == \"2017\":\n",
    "            AL_table = parse_html(AL_table_html, is_wildcard)\n",
    "            NL_table = parse_html(NL_table_html, is_wildcard)\n",
    "        elif year == \"2018\":\n",
    "            AL_table = parse_html_2018(AL_table_html, is_wildcard)\n",
    "            NL_table = parse_html_2018(NL_table_html, is_wildcard)\n",
    "        elif year == \"2022\":\n",
    "            AL_table = parse_html_2022(AL_table_html, is_wildcard)\n",
    "            NL_table = parse_html_2022(NL_table_html, is_wildcard)\n",
    "        total = AL_table + NL_table\n",
    "        if is_wildcard:\n",
    "            res = {x[0]: x for x in total}\n",
    "        else:\n",
    "            res = {x[1]: x for x in total}\n",
    "        \n",
    "        \n",
    "        return res\n",
    "    except:\n",
    "        raise ValueError\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e9f0151-934f-46ca-8e2f-c3f2bd7e5c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Blue Jays': [' ', 'Blue Jays', ' ', ' ', '0', '0', '0.0', ' ', '43.8%', '45.8%', '89.6%'], 'Yankees': [' ', 'Yankees', ' ', ' ', '0', '0', '0.0', ' ', '34.6%', '51.2%', '85.8%'], 'Astros': [' ', 'Astros', ' ', ' ', '0', '0', '0.0', ' ', '71.3%', '14.4%', '85.7%'], 'White Sox': [' ', 'White Sox', ' ', ' ', '0', '0', '0.0', ' ', '59.2%', '12.9%', '72.2%'], 'Red Sox': [' ', 'Red Sox', ' ', ' ', '0', '0', '0.0', ' ', '12.9%', '48.4%', '61.3%'], 'Rays': [' ', 'Rays', ' ', ' ', '0', '0', '0.0', ' ', '8.7%', '44.1%', '52.7%'], 'Angels': [' ', 'Angels', ' ', ' ', '0', '0', '0.0', ' ', '18.5%', '26.2%', '44.7%'], 'Twins': [' ', 'Twins', ' ', ' ', '0', '0', '0.0', ' ', '23.3%', '17.2%', '40.4%'], 'Mariners': [' ', 'Mariners', ' ', ' ', '0', '0', '0.0', ' ', '7.7%', '15.1%', '22.8%'], 'Guardians': [' ', 'Guardians', ' ', ' ', '0', '0', '0.0', ' ', '7.5%', '7.6%', '15.2%'], 'Tigers': [' ', 'Tigers', ' ', ' ', '0', '0', '0.0', ' ', '5.9%', '6.3%', '12.1%'], 'Royals': [' ', 'Royals', ' ', ' ', '0', '0', '0.0', ' ', '4.1%', '4.4%', '8.4%'], 'Rangers': [' ', 'Rangers', ' ', ' ', '0', '0', '0.0', ' ', '2.2%', '5.7%', '7.9%'], 'Athletics': [' ', 'Athletics', ' ', ' ', '0', '0', '0.0', ' ', '0.3%', '0.8%', '1.1%'], 'Orioles': [' ', 'Orioles', ' ', ' ', '0', '0', '0.0', ' ', '0.0%', '0.1%', '0.1%'], 'Dodgers': [' ', 'Dodgers', ' ', ' ', '0', '0', '0.0', ' ', '65.0%', '28.3%', '93.3%'], 'Braves': [' ', 'Braves', ' ', ' ', '0', '0', '0.0', ' ', '53.8%', '33.9%', '87.7%'], 'Brewers': [' ', 'Brewers', ' ', ' ', '0', '0', '0.0', ' ', '72.0%', '9.3%', '81.3%'], 'Padres': [' ', 'Padres', ' ', ' ', '0', '0', '0.0', ' ', '25.4%', '50.3%', '75.7%'], 'Mets': [' ', 'Mets', ' ', ' ', '0', '0', '0.0', ' ', '23.1%', '44.6%', '67.8%'], 'Phillies': [' ', 'Phillies', ' ', ' ', '0', '0', '0.0', ' ', '17.7%', '43.1%', '60.9%'], 'Giants': [' ', 'Giants', ' ', ' ', '0', '0', '0.0', ' ', '9.5%', '38.7%', '48.3%'], 'Cardinals': [' ', 'Cardinals', ' ', ' ', '0', '0', '0.0', ' ', '20.8%', '17.5%', '38.2%'], 'Marlins': [' ', 'Marlins', ' ', ' ', '0', '0', '0.0', ' ', '5.2%', '23.9%', '29.1%'], 'Reds': [' ', 'Reds', ' ', ' ', '0', '0', '0.0', ' ', '3.7%', '4.2%', '7.9%'], 'Cubs': [' ', 'Cubs', ' ', ' ', '0', '0', '0.0', ' ', '3.1%', '3.4%', '6.5%'], 'Nationals': [' ', 'Nationals', ' ', ' ', '0', '0', '0.0', ' ', '0.1%', '1.2%', '1.3%'], 'Pirates': [' ', 'Pirates', ' ', ' ', '0', '0', '0.0', ' ', '0.4%', '0.4%', '0.9%'], 'Diamondbacks': [' ', 'Diamondbacks', ' ', ' ', '0', '0', '0.0', ' ', '0.0%', '0.7%', '0.7%'], 'Rockies': [' ', 'Rockies', ' ', ' ', '0', '0', '0.0', ' ', '0.0%', '0.4%', '0.4%']}\n",
      "2022-04-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\"testing\"\"\"\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "driver.get(url_lg[2929])\n",
    "lg_data = load_page(driver, False, \"2022\")\n",
    "driver.get(url_wc[2929])\n",
    "wc_data = load_page(driver, True, \"2022\")\n",
    "print(lg_data)\n",
    "print(date_list[2929])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8710c470-24f4-4f1f-88f3-3f4186ec6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(total):\n",
    "    df = pd.DataFrame(total[1:], columns=headers)\n",
    "    df['game'] = df['game'].astype(int)\n",
    "    \n",
    "    # Add missing games\n",
    "    new_rows = []\n",
    "    grouped = df.groupby(['year', 'team'])\n",
    "    \n",
    "    for (year, team), group in grouped:\n",
    "        existing_games = set(group['game'])\n",
    "        min_game = min(existing_games)\n",
    "        max_game = max(existing_games)\n",
    "        \n",
    "        for game in range(min_game, max_game + 1):\n",
    "            if game not in existing_games:\n",
    "                new_row = [year, team, game, ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ']\n",
    "                new_rows.append(new_row)\n",
    "    \n",
    "    new_df = pd.DataFrame(new_rows, columns=headers)\n",
    "    \n",
    "    # Concatenate original and new rows\n",
    "    df_updated = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Prioritize non-empty GBWC\n",
    "    df_updated['GBWC'] = df_updated['GBWC'].fillna('')\n",
    "    df_updated = df_updated.sort_values(by=['year', 'team', 'game', 'GBWC'], ascending=[True, True, True, False])\n",
    "    \n",
    "    # Remove duplicates, keeping the first occurrence (which has a non-empty GBWC if possible)\n",
    "    df_updated = df_updated.drop_duplicates(subset=['year', 'team', 'game'], keep='first')\n",
    "    \n",
    "    df_updated = df_updated.sort_values(by=[\"year\", \"team\", \"game\"]).reset_index(drop=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0072ec69-60e8-4bd7-afbc-8f0f85207bea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(date, url_wc, url_lg, year):\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    try:\n",
    "        driver.get(url_wc)\n",
    "        wc_data = load_page(driver, True, year)\n",
    "\n",
    "        driver.get(url_lg)\n",
    "        lg_data = load_page(driver, False, year)\n",
    "        combined_data = combine(lg_data, wc_data, date)\n",
    "        return list(combined_data.values())\n",
    "    except Exception as e:\n",
    "        print(f'{str(e)} Error on date: {date}')\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e3ce5d-ac2c-4864-a369-9dcf14860bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_reading(year, start, end, writeto, total = []):\n",
    "\n",
    "    for i in range(start, end):\n",
    "        df = clean(total)\n",
    "        df.to_csv(\"L:/RA_work/JAY/datasets/final/2/\" + str(writeto) + \".csv\", index=False, header=headers)\n",
    "        print(f\"On {date_list[i]}\")\n",
    "        try:\n",
    "            total.extend(run(date_list[i], url_wc[i], url_lg[i], year))\n",
    "        except:\n",
    "            continue\n",
    "    df = clean(total)\n",
    "    df.to_csv(\"L:/RA_work/JAY/datasets/final/2/\" + str(writeto) + \".csv\", index=False, header=headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "406b8f54-4461-4f84-8cda-83b1fd709b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_progress(path) -> list:\n",
    "    total = []\n",
    "    with open(path, mode='r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        \n",
    "        for row in reader:\n",
    "            total.append(row)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fb12141-a6b0-4806-a709-265113db15ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-28\n",
      "2018-05-03\n"
     ]
    }
   ],
   "source": [
    "print(date_list[1459])\n",
    "print(date_list[1495])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af66102c-fc6d-4b50-b5dd-5ad89cdefe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 2018-03-28\n"
     ]
    }
   ],
   "source": [
    "\"\"\"RUN THIS TO START\"\"\"\n",
    "\n",
    "pre2018 = [250, 500, 750, 1000, 1250] # end at 1313\n",
    "post2018 = [1500, 1750, 2000, 2250, 2500, 2750] # start at 1459, end at 2776\n",
    "post2022 = [3000, 3250, 3500, 3750] # start at 2929, end at 3819\n",
    "\n",
    "#total = []\n",
    "#start_reading(\"2018\", 1459, 1500, total)\n",
    "# read up to end of 2017\n",
    "#for i in pre2018:\n",
    "    #total = []\n",
    "    #start_reading(\"2017\", i - 250, i, total)\n",
    "#total = convert_progress(\"L:/RA_work/JAY/datasets/final/2/1250.csv\")\n",
    "#start_reading(\"2017\", 1100, 1250, total)\n",
    "#total = []\n",
    "#start_reading(\"2017\", 1250, 1313, total)\n",
    "\n",
    "# read up to end of 2021\n",
    "#total = convert_progress(\"L:/RA_work/JAY/datasets/final/2/1500.csv\")\n",
    "#start_reading(\"2018\", 1491, 1500, total)\n",
    "#post2018 = [2500, 2750]\n",
    "#total = convert_progress(\"L:/RA_work/JAY/datasets/final/2/2250.csv\")\n",
    "#start_reading(\"2018\", 2173, 2250, total)\n",
    "#for i in post2018:\n",
    "    #total = []\n",
    "    #start_reading(\"2018\", i - 250, i, total)\n",
    "#total = convert_progress(\"L:/RA_work/JAY/datasets/final/2/2776.csv\")\n",
    "\n",
    "#start_reading(\"2018\", 2752, 2776, total)\n",
    "# read up to end of 2022\n",
    "total = convert_progress(\"L:/RA_work/JAY/datasets/final/2/1500.csv\")\n",
    "start_reading(\"2018\", 1459, 1460, 1500, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9547be08-9277-41b2-ad00-b04d15998d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-15\n"
     ]
    }
   ],
   "source": [
    "print(date_list[2999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93d3fefd-2206-4b7b-8cfb-10f4c271b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Read all CSVs\n",
    "all_files = glob.glob(\"L:/RA_work/JAY/datasets/final/2/*.csv\")\n",
    "\n",
    "# Merge all CSV files\n",
    "df = pd.concat([pd.read_csv(f) for f in all_files])\n",
    "\n",
    "# Apply the clean function\n",
    "df_cleaned = clean(df.values.tolist())\n",
    "\n",
    "# Save the cleaned and sorted CSV\n",
    "df_cleaned.to_csv(\"L:/RA_work/JAY/datasets/final/merged_sorted_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8aa4ef-b8cc-4672-b068-0d3832a3c82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
